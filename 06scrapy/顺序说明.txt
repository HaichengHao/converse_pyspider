## 顺序说明  
最开始就是按照英文顺序，可以关注一下第六个imagespipline,然后看第七个实例
之后就是deepcrawl和deeppro
再之后为baidufanyi用来补全scrapy的post请求
再之后就是中间件middleware相关的,  
之后是selenium+scrapy(wangyinews)还有wangyipro


之后就是新的scrapy补充内容,有的很基础，先是game
然后是ciapiao 这部分需要重点关注HtmlResponse(url=model_urls[0],encoding='utf-8', body=page_text),将得到的数据封装成真正的response对象
之后是对应之前的imgpro而新建立的tpzj

之后是比较重要的
处理cookie的shujia,注意，这里对之前的中间件部分相比不同之处有很多，包括init文件中对于cookie
的定义是字典，所以要处理cookie
还有一种是直接进行模拟登录的第二套方案，不需要cookie,其实就是post请求传参FormRequest(url=,callback=,formdata),和之前的baidufanyi很像
cjy是实现验证码识别登录的，算是补充

然后是新的middleware的详解的midmid项目,注意可以看midmid权重设置后的影响
对于process_request来说，权重小的先执行，对于process_response来说,权重大的先执行
所以才会是下面这样
(converse_pyspider) E:\converse_spider\converse_pyspider\06scrapy\midmid>scrapy crawl midw
Mozilla/5.0 (iPhone; CPU iPhone OS 18_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.2 Mobile/15E148 Safari/604.1
我是spider_opened
我是proecess_request <--权重小的先执行
我是proecess_request2
我是process_reqponse2 <--权重大的先执行
我是process_reqponse

之后是movierank，强化中间件中如ua和代理的配置
然后是boss,这个和之前写的caipiao有相似之处，不过更加规范，并且其中加入了对请求的自定义，封装了Selenium请求并加入了isinstanse()
还有就是page_source是字符串类型的，如果想要转换为scrapy能够处理的response类型需要from scrapy.http import HtmlResponse ，
然后用HtmlResponse(url=,body=page_source,encoding=,request=request)来进行封装并return

然后是汽车之家qichezhijia,补充了新知识，即csv文件写入表头
 self.fp=open('carinfo.csv','a+',encoding='utf-8',newline='')
        self.writer=csv.writer(self.fp)
        # 检查表头是否为空，如果是空的就写入表头!!
        if self.fp.tell() == 0:
            self.writer.writerow(['Car Title', 'Price', 'Licheng', 'Shijian', 'Leixing', 'Area', 'Guobiao'])


然后是wangxiao，用到了链接提取器,关注请求的构造以及如何通过子节点拿到父节点,还有就是构造的请求headers中必须要填充的如x-requested-with:和user-agent等,
还有要加的就是请求的数据是json所以要加上content-type
其中x-requested-with是ajax这种请求中常常含有的参数
之后做那种页面链接不变但是内容改变的网站对其发送请求的时候也可以利用这种思路
