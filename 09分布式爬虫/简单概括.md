## 在settings文件中要进行的操作
```python
ITEM_PIPELINES = {
    "scrapy_redis.pipelines.RedisPipeline": 301,
    "second.pipelines.SecondPipeline": 300,

}

REDIS_HOST = "127.0.0.1"
REDIS_PORT = 6379
REDIS_DB = 14

# tips: 使用scrapy-redis组件的过滤器去重队列
# DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
# tips :使用scrapy-redis组件自己的调度器
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
# tips: 如果为真，在关闭时自动保存请求信息，如果为假，则不保存请求信息
SCHEDULER_PERSIST = True #tips:设置为True可以实现断点续爬

# important:使用布隆过滤器
# 在去重类中使用BloomFilter 替换 DUPEFILTER_CLASS便可使用
DUPEFILTER_CLASS = "scrapy_redis_bloomfilter.dupefilter.RFPDupeFilter"
# 哈希函数的个数，默认为6，可以自行修改
BLOOMFILTER_HASH_MEMBER = 6
# BloomFilter的bit参数，默认30，占用128M空间，去重量级1亿
BLOOMFILTER_BIT = 30
```   

### 在爬虫程序中要进行的操作  
```python
import scrapy
from scrapy_redis.spiders import RedisSpider <--引入了RedisSpider
from redis2 import Redis
from ..items import SecondItem
class TyzxSpider(RedisSpider): <--这里继承自RedisSpider
    conn = Redis(
        host='localhost',
        port=6379,
        db=14
    )
    name = "tyzx"
    # allowed_domains = ["www.xxx.com"]
    # start_urls = ["https://www.xxx.com"]
    redis_key = "tyzx_start_url"
    model_url = 'https://xintianya.net/index-%d.htm'
    page_number = 2

    def parse(self, response):
        urls = response.xpath(
            '//div[@class="card card-threadlist"]//ul/li/div[@class="media-body"]/div/a/@href').extract()
        for url in urls:
            detail_url = 'https://xintianya.net/' + url
            print(detail_url)
            # tips:和之前不同的是，这里不用判断是否出现重复,所有的判断工作，全都交给scrapy-redis完成
            yield scrapy.Request(
                url=detail_url,
                callback=self.parse_detail
            )

        if self.page_number < 15:
            new_url = self.model_url % self.page_number
            self.page_number += 1
            yield scrapy.Request(url=new_url, callback=self.parse)
    def parse_detail(self, response):
        usrname = response.xpath('//a[@class="text-muted"]/b/text()').extract_first()
        title = response.xpath('//span[@class="break-all"]/text()').extract_first().strip()
        # tips:实例化item对象
        item = SecondItem()
        item['usrname'] = usrname
        item['title'] = title
        # self.redis.sadd("tianya:ty:detail:url", response.url)
        yield item


```